{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a532142",
   "metadata": {},
   "source": [
    "# GAN Kaggle Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef657a1f-d6ac-40b0-b9d9-bf84517a430e",
   "metadata": {},
   "source": [
    "## GAN - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb9a7f-1433-4220-bec1-271bd45272e7",
   "metadata": {},
   "source": [
    "#### Citation: https://arxiv.org/abs/1406.2661  \n",
    "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f2b73-6e60-40da-ac06-a2ebe8447e03",
   "metadata": {},
   "source": [
    "## Problem / Kaggle Competition: \"I’m Something of a Painter Myself\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e53ba-44b5-4304-a0ff-9a27a20ecd2a",
   "metadata": {},
   "source": [
    "#### Citation: Kaggle - https://www.kaggle.com/competitions/gan-getting-started/overview\n",
    "A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.  \n",
    "The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.  \n",
    "Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed458d7d-0543-439d-8c51-9b5cd6b37783",
   "metadata": {},
   "source": [
    "#### Kaggle score: https://www.kaggle.com/competitions/gan-getting-started/leaderboard\n",
    "- FID measures how close your generated images are to real images.\n",
    "- Lower FID = better quality\n",
    "- 0 means perfect match (never happens in practice)\n",
    "- So on the leaderboard: Lower score = Better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c20a7f-4b1b-483d-bec4-625a4f0e57aa",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87806cd3-f257-4fb2-9b96-1078297daf32",
   "metadata": {},
   "source": [
    "#### Citation: Kaggle - https://www.kaggle.com/competitions/gan-getting-started/data\n",
    "Dataset  \n",
    "The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.  \n",
    "The monet directories contain Monet paintings. Use these images to train your model.  \n",
    "The photo directories contain photos. Add Monet-style to these images and submit your generated jpeg images as a zip file. Other photos outside of this dataset can be transformed but keep your submission file limited to 10,000 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d8c48-c0ce-4c51-9a61-d0728a5ab7bf",
   "metadata": {},
   "source": [
    "## Plan\n",
    "- Import Libraries\n",
    "- Setup configuration\n",
    "- Check the data\n",
    "- Define Functions\n",
    "- Load the data\n",
    "- EDA\n",
    "- Model Setup\n",
    "- Model Training\n",
    "- Generate file for Kaggle submission\n",
    "- Submission\n",
    "- Conclusions\n",
    "- Discussion\n",
    "- Link GitHub Repository\n",
    "- Citation\n",
    "- AI Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd249f-181e-43d2-8d12-ddcb42308bbc",
   "metadata": {},
   "source": [
    "## Notebook\n",
    "- The Notebook is setup to run in the Kaggle environment.\n",
    "- Set \"Accelerator\" to \"GPU T4x2\" in the Kaggle environment.\n",
    "- A similar notebook was used in Kaggle to submit to Kaggle as \"notebook03b8491926 - Version3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77efad9-3d91-4e03-98fa-8e7aa8141d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Disable XLA (can be unstable on Kaggle GPU combos)\n",
    "tf.config.optimizer.set_jit(False)\n",
    "print(\"XLA disabled.\")\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Use default strategy \n",
    "strategy = tf.distribute.get_strategy()\n",
    "print(\"Using default strategy\")\n",
    "print(\"Replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "IMG_HEIGHT, IMG_WIDTH = IMAGE_SIZE\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 1 * strategy.num_replicas_in_sync     # Up to 4\n",
    "EPOCHS = 10                                        \n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "LR = 2e-4\n",
    "LAMBDA_CYCLE = 10.0\n",
    "LAMBDA_ID = 0.5 * LAMBDA_CYCLE\n",
    "\n",
    "N_GENERATED_IMAGES = 7000\n",
    "\n",
    "COMPETITION_NAME = \"gan-getting-started\"\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path(COMPETITION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6143b",
   "metadata": {},
   "source": [
    "## Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "def count_data_items(filenames):\n",
    "    counts = []\n",
    "    for f in filenames:\n",
    "        m = re.search(r\"-([0-9]+)\\.tfrec$\", f)\n",
    "        if m:\n",
    "            counts.append(int(m.group(1)))\n",
    "    if counts:\n",
    "        return int(np.sum(counts))\n",
    "    return len(filenames)\n",
    "\n",
    "MONET_TFRECS = tf.io.gfile.glob(os.path.join(GCS_PATH, \"monet_tfrec\", \"*.tfrec\"))\n",
    "PHOTO_TFRECS = tf.io.gfile.glob(os.path.join(GCS_PATH, \"photo_tfrec\", \"*.tfrec\"))\n",
    "\n",
    "print(\"Monet TFRecord files:\", len(MONET_TFRECS))\n",
    "print(\"Photo TFRecord files:\", len(PHOTO_TFRECS))\n",
    "\n",
    "if len(MONET_TFRECS) == 0 or len(PHOTO_TFRECS) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        \"TFRecord files not found. Make sure the 'gan-getting-started' \"\n",
    "        \"dataset is attached to this Kaggle notebook.\"\n",
    "    )\n",
    "\n",
    "n_monet = count_data_items(MONET_TFRECS)\n",
    "n_photo = count_data_items(PHOTO_TFRECS)\n",
    "\n",
    "print(\"Approx Monet images:\", n_monet)\n",
    "print(\"Approx Photo images:\", n_photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca3876-a1e9-4872-ae37-c6aa1bfecedf",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24996003-ffd0-4927-8ecb-57ad6f6cd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_bytes):\n",
    "    image = tf.image.decode_jpeg(image_bytes, channels=CHANNELS)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = image * 2.0 - 1.0\n",
    "    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example_proto):\n",
    "    feature_description = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    image = decode_image(example[\"image\"])\n",
    "    return image\n",
    "\n",
    "def augment_image(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    new_size = tf.random.uniform([], minval=256, maxval=286, dtype=tf.int32)\n",
    "    image = tf.image.resize(image, [new_size, new_size])\n",
    "    image = tf.image.random_crop(image, [IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "    return image\n",
    "\n",
    "def load_dataset(filenames, augment=False, shuffle=False, repeat=False, batch_size=1):\n",
    "    ds = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    ds = ds.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "    if augment:\n",
    "        ds = ds.map(augment_image, num_parallel_calls=AUTO)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61544dbe-e569-4d6c-a50f-52dd7c50bd25",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f94ea-2397-4d2e-9963-65b928c14158",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_ds_train = load_dataset(\n",
    "    MONET_TFRECS,\n",
    "    augment=True,\n",
    "    shuffle=True,\n",
    "    repeat=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "photo_ds_train = load_dataset(\n",
    "    PHOTO_TFRECS,\n",
    "    augment=True,\n",
    "    shuffle=True,\n",
    "    repeat=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.zip((photo_ds_train, monet_ds_train))\n",
    "\n",
    "STEPS_PER_EPOCH = min(n_monet, n_photo) // BATCH_SIZE\n",
    "print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "if STEPS_PER_EPOCH == 0:\n",
    "    raise ValueError(\"STEPS_PER_EPOCH is 0. Reduce BATCH_SIZE or check data.\")\n",
    "\n",
    "photo_ds_inference = load_dataset(\n",
    "    PHOTO_TFRECS,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    repeat=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a80f6",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e4ae4-d0d2-4d59-9209-7ebc0b015f43",
   "metadata": {},
   "source": [
    "- Visial inspection of samples\n",
    "- Check width distribution\n",
    "- Show pixel-intensity histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9b4b6-6517-4efe-b36b-87267ee55823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Inspection Samples\n",
    "def show_samples(paths, title, n=6):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    chosen = random.sample(paths, min(n, len(paths)))\n",
    "    for i, p in enumerate(chosen):\n",
    "        img = Image.open(p)\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(os.path.basename(p))\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "monet_files = sorted(tf.io.gfile.glob(\"/kaggle/input/gan-getting-started/monet_jpg/*.jpg\"))\n",
    "photo_files = sorted(tf.io.gfile.glob(\"/kaggle/input/gan-getting-started/photo_jpg/*.jpg\"))\n",
    "show_samples(monet_files, \"Random Monet Images\")\n",
    "show_samples(photo_files, \"Random Photo Images\")\n",
    "\n",
    "# Metadata Statistics\n",
    "def compute_metadata(paths, sample_size=200):\n",
    "    widths, heights = [], []\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    sample = random.sample(paths, min(sample_size, len(paths)))\n",
    "\n",
    "    for p in tqdm(sample, desc=\"Metadata\"):\n",
    "        img = np.array(Image.open(p).convert(\"RGB\"))\n",
    "        h, w, _ = img.shape\n",
    "        heights.append(h)\n",
    "        widths.append(w)\n",
    "        means.append(img.mean())\n",
    "        stds.append(img.std())\n",
    "\n",
    "    return widths, heights, means, stds\n",
    "\n",
    "monet_w, monet_h, monet_mean, monet_std = compute_metadata(monet_files)\n",
    "photo_w, photo_h, photo_mean, photo_std = compute_metadata(photo_files)\n",
    "\n",
    "# Plot dimension distributions\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(monet_w, bins=20, alpha=0.5, label=\"Monet width\")\n",
    "plt.hist(photo_w, bins=20, alpha=0.5, label=\"Photo width\")\n",
    "plt.legend()\n",
    "plt.title(\"Image Width Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(monet_mean, bins=20, alpha=0.5, label=\"Monet pixel mean\")\n",
    "plt.hist(photo_mean, bins=20, alpha=0.5, label=\"Photo pixel mean\")\n",
    "plt.legend()\n",
    "plt.title(\"Pixel Intensity Means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b2b03-654a-4286-9b83-f74ab34a121a",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "- The width distribution indicates that all images are uniformly sized, so no resizing issues are present.\n",
    "- Pixel-intensity histograms reveal that Monet images tend to have slightly higher and more widely spread brightness values.\n",
    "  The photos cluster is more tightly at lower intensities, reflecting their more balanced, natural lighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a396f9",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "- InstanceNorm: Normalizes each sample independently across spatial dimensions\n",
    "- Residual Block: A pair of convolutional layers with a skip connection that lets the input flow directly to the output\n",
    "- ResNet Generator: A generator built from several residual blocks, allowing it to learn complex transformations\n",
    "- Discriminator: A CNN that judges whether an image is real or generated, guiding the generator by providing feedback on how realistic its outputs appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa05b75-2712-43ca-8dd5-b33067948b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InstanceNorm\n",
    "class InstanceNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon=1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(channels,), initializer=\"ones\", trainable=True, name=\"gamma\"\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(channels,), initializer=\"zeros\", trainable=True, name=\"beta\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        inv = tf.math.rsqrt(var + self.epsilon)\n",
    "        x_norm = (x - mean) * inv\n",
    "        return self.gamma * x_norm + self.beta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684f6ed-eb0b-478f-86c2-341c16c32b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "def residual_block(x, filters, use_norm=True, name=None):\n",
    "    init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    skip = x\n",
    "\n",
    "    y = layers.Conv2D(\n",
    "        filters, 3, strides=1, padding=\"same\",\n",
    "        kernel_initializer=init, use_bias=not use_norm,\n",
    "        name=None if name is None else name + \"_conv1\",\n",
    "    )(x)\n",
    "    if use_norm:\n",
    "        y = InstanceNormalization(name=None if name is None else name + \"_in1\")(y)\n",
    "    y = layers.ReLU()(y)\n",
    "\n",
    "    y = layers.Conv2D(\n",
    "        filters, 3, strides=1, padding=\"same\",\n",
    "        kernel_initializer=init, use_bias=not use_norm,\n",
    "        name=None if name is None else name + \"_conv2\",\n",
    "    )(y)\n",
    "    if use_norm:\n",
    "        y = InstanceNormalization(name=None if name is None else name + \"_in2\")(y)\n",
    "\n",
    "    out = layers.Add(name=None if name is None else name + \"_add\")([skip, y])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00454a57-efe7-44f2-a305-f79738aacab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Generator\n",
    "def build_resnet_generator(name=\"generator\", n_res_blocks=9):\n",
    "    init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        64, 7, strides=1, padding=\"same\",\n",
    "        kernel_initializer=init, use_bias=False,\n",
    "    )(inputs)\n",
    "    x = InstanceNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    for filters in [128, 256]:\n",
    "        x = layers.Conv2D(\n",
    "            filters, 3, strides=2, padding=\"same\",\n",
    "            kernel_initializer=init, use_bias=False,\n",
    "        )(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    for i in range(n_res_blocks):\n",
    "        x = residual_block(x, 256, name=f\"res{i+1}\")\n",
    "\n",
    "    for filters in [128, 64]:\n",
    "        x = layers.Conv2DTranspose(\n",
    "            filters, 3, strides=2, padding=\"same\",\n",
    "            kernel_initializer=init, use_bias=False,\n",
    "        )(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        CHANNELS, 7, strides=1, padding=\"same\",\n",
    "        kernel_initializer=init,\n",
    "    )(x)\n",
    "    outputs = layers.Activation(\"tanh\")(x)\n",
    "    return keras.Model(inputs, outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285187b2-c1fd-482d-8a7a-8d20b9dc621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def build_discriminator(name=\"discriminator\"):\n",
    "    init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))\n",
    "\n",
    "    def disc_block(x, filters, stride, use_norm=True):\n",
    "        x = layers.Conv2D(\n",
    "            filters, 4, strides=stride, padding=\"same\",\n",
    "            kernel_initializer=init, use_bias=not use_norm,\n",
    "        )(x)\n",
    "        if use_norm:\n",
    "            x = InstanceNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        return x\n",
    "\n",
    "    x = disc_block(inputs, 64, stride=2, use_norm=False)\n",
    "    x = disc_block(x, 128, stride=2)\n",
    "    x = disc_block(x, 256, stride=2)\n",
    "    x = disc_block(x, 512, stride=1)\n",
    "    x = layers.Conv2D(1, 4, strides=1, padding=\"same\", kernel_initializer=init)(x)\n",
    "    return keras.Model(inputs, x, name=name)\n",
    "\n",
    "with strategy.scope():\n",
    "    G_photo_to_monet = build_resnet_generator(name=\"G_photo_to_monet\")\n",
    "    G_monet_to_photo = build_resnet_generator(name=\"G_monet_to_photo\")\n",
    "\n",
    "    D_monet = build_discriminator(name=\"D_monet\")\n",
    "    D_photo = build_discriminator(name=\"D_photo\")\n",
    "\n",
    "    mse_loss = keras.losses.MeanSquaredError()\n",
    "    mae_loss = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def generator_adversarial_loss(fake_logits):\n",
    "        return mse_loss(tf.ones_like(fake_logits), fake_logits)\n",
    "\n",
    "    def discriminator_loss(real_logits, fake_logits):\n",
    "        real_loss = mse_loss(tf.ones_like(real_logits), real_logits)\n",
    "        fake_loss = mse_loss(tf.zeros_like(fake_logits), fake_logits)\n",
    "        return 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "    def cycle_consistency_loss(real, cycled):\n",
    "        return mae_loss(real, cycled)\n",
    "\n",
    "    def identity_loss(real, same):\n",
    "        return mae_loss(real, same)\n",
    "\n",
    "    gen_G_optimizer = keras.optimizers.Adam(LR, beta_1=0.5, beta_2=0.999)\n",
    "    gen_F_optimizer = keras.optimizers.Adam(LR, beta_1=0.5, beta_2=0.999)\n",
    "    disc_monet_optimizer = keras.optimizers.Adam(LR, beta_1=0.5, beta_2=0.999)\n",
    "    disc_photo_optimizer = keras.optimizers.Adam(LR, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1001b-9375-44a5-a053-fde4f8537b34",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b040a-bca8-4d88-a179-1e0d08e7d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "def train_step(batch):\n",
    "    real_photo, real_monet = batch\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_monet = G_photo_to_monet(real_photo, training=True)\n",
    "        fake_photo = G_monet_to_photo(real_monet, training=True)\n",
    "\n",
    "        cycled_photo = G_monet_to_photo(fake_monet, training=True)\n",
    "        cycled_monet = G_photo_to_monet(fake_photo, training=True)\n",
    "\n",
    "        same_monet = G_photo_to_monet(real_monet, training=True)\n",
    "        same_photo = G_monet_to_photo(real_photo, training=True)\n",
    "\n",
    "        disc_real_monet = D_monet(real_monet, training=True)\n",
    "        disc_fake_monet = D_monet(fake_monet, training=True)\n",
    "        disc_real_photo = D_photo(real_photo, training=True)\n",
    "        disc_fake_photo = D_photo(fake_photo, training=True)\n",
    "\n",
    "        gen_G_adv = generator_adversarial_loss(disc_fake_monet)\n",
    "        gen_F_adv = generator_adversarial_loss(disc_fake_photo)\n",
    "\n",
    "        cycle_loss_photo = cycle_consistency_loss(real_photo, cycled_photo)\n",
    "        cycle_loss_monet = cycle_consistency_loss(real_monet, cycled_monet)\n",
    "        total_cycle_loss = cycle_loss_photo + cycle_loss_monet\n",
    "\n",
    "        id_loss_monet = identity_loss(real_monet, same_monet)\n",
    "        id_loss_photo = identity_loss(real_photo, same_photo)\n",
    "\n",
    "        total_gen_G_loss = gen_G_adv + LAMBDA_CYCLE * total_cycle_loss + LAMBDA_ID * id_loss_monet\n",
    "        total_gen_F_loss = gen_F_adv + LAMBDA_CYCLE * total_cycle_loss + LAMBDA_ID * id_loss_photo\n",
    "\n",
    "        disc_monet_loss = discriminator_loss(disc_real_monet, disc_fake_monet)\n",
    "        disc_photo_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n",
    "\n",
    "    gen_G_gradients = tape.gradient(total_gen_G_loss, G_photo_to_monet.trainable_variables)\n",
    "    gen_F_gradients = tape.gradient(total_gen_F_loss, G_monet_to_photo.trainable_variables)\n",
    "    disc_monet_gradients = tape.gradient(disc_monet_loss, D_monet.trainable_variables)\n",
    "    disc_photo_gradients = tape.gradient(disc_photo_loss, D_photo.trainable_variables)\n",
    "\n",
    "    gen_G_optimizer.apply_gradients(zip(gen_G_gradients, G_photo_to_monet.trainable_variables))\n",
    "    gen_F_optimizer.apply_gradients(zip(gen_F_gradients, G_monet_to_photo.trainable_variables))\n",
    "    disc_monet_optimizer.apply_gradients(zip(disc_monet_gradients, D_monet.trainable_variables))\n",
    "    disc_photo_optimizer.apply_gradients(zip(disc_photo_gradients, D_photo.trainable_variables))\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return {\n",
    "        \"gen_G_loss\": float(total_gen_G_loss.numpy()),\n",
    "        \"gen_F_loss\": float(total_gen_F_loss.numpy()),\n",
    "        \"disc_monet_loss\": float(disc_monet_loss.numpy()),\n",
    "        \"disc_photo_loss\": float(disc_photo_loss.numpy()),\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "from time import time\n",
    "\n",
    "history = {\n",
    "    \"gen_G_loss\": [],\n",
    "    \"gen_F_loss\": [],\n",
    "    \"disc_monet_loss\": [],\n",
    "    \"disc_photo_loss\": [],\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time()\n",
    "    epoch_metrics = {\n",
    "        \"gen_G_loss\": 0.0,\n",
    "        \"gen_F_loss\": 0.0,\n",
    "        \"disc_monet_loss\": 0.0,\n",
    "        \"disc_photo_loss\": 0.0,\n",
    "    }\n",
    "    for step, batch in enumerate(train_ds.take(STEPS_PER_EPOCH)):\n",
    "        metrics = train_step(batch)\n",
    "        for k in epoch_metrics:\n",
    "            epoch_metrics[k] += metrics[k]\n",
    "\n",
    "        if (step + 1) % 10 == 0 or (step + 1) == STEPS_PER_EPOCH:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{EPOCHS}] \"\n",
    "                f\"Step [{step+1}/{STEPS_PER_EPOCH}] \"\n",
    "                f\"G_G: {metrics['gen_G_loss']:.2f} \"\n",
    "                f\"G_F: {metrics['gen_F_loss']:.2f} \"\n",
    "                f\"D_M: {metrics['disc_monet_loss']:.2f} \"\n",
    "                f\"D_P: {metrics['disc_photo_loss']:.2f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    epoch_time = time() - start_time\n",
    "    for k in epoch_metrics:\n",
    "        epoch_metrics[k] /= STEPS_PER_EPOCH\n",
    "        history[k].append(epoch_metrics[k])\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch}/{EPOCHS} time: {epoch_time:.1f}s | \"\n",
    "        f\"G_G: {epoch_metrics['gen_G_loss']:.3f} \"\n",
    "        f\"G_F: {epoch_metrics['gen_F_loss']:.3f} \"\n",
    "        f\"D_M: {epoch_metrics['disc_monet_loss']:.3f} \"\n",
    "        f\"D_P: {epoch_metrics['disc_photo_loss']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6398e-05eb-4d25-a73e-a43924e8daaa",
   "metadata": {},
   "source": [
    "## Generate file for Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8de0e7-1dc2-4223-89ab-5dff3a3072b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Monet-style images into images.zip\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "ZIP_PATH = OUTPUT_DIR / \"images.zip\"\n",
    "\n",
    "def denormalize_to_uint8(image):\n",
    "    image = (image + 1.0) * 0.5\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
    "    return image\n",
    "\n",
    "print(\"Generating Monet-style images into images.zip ...\")\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    idx = 0\n",
    "    for batch in photo_ds_inference:\n",
    "        if idx >= N_GENERATED_IMAGES:\n",
    "            break\n",
    "        photo = batch\n",
    "        fake_monet = G_photo_to_monet(photo, training=False)[0]\n",
    "        img_uint8 = denormalize_to_uint8(fake_monet)\n",
    "        jpg_bytes = tf.io.encode_jpeg(img_uint8).numpy()\n",
    "\n",
    "        filename = f\"image_{idx:05d}.jpg\"\n",
    "        zf.writestr(filename, jpg_bytes)\n",
    "        idx += 1\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Generated {idx}/{N_GENERATED_IMAGES} images...\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\nDone. Wrote {idx} images to {ZIP_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37e4d1-319c-427b-9492-67799fc6273d",
   "metadata": {},
   "source": [
    "## Submission Results\n",
    "Kaggle score \"notebook03b8491926 - Version3\": 84.30637  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45072e-27fe-4c9f-bff4-577e90659536",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusions\n",
    "- The GAN pipeline successfully generated Monet-style images and achieved a competitive score on the Kaggle leaderboard.\n",
    "- The training process demonstrated stable adversarial learning.\n",
    "- The results suggest that further gains are possible through hyperparameter tuning—such as adjusting learning rates, experimenting with alternative GAN losses, or increasing model capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c65984-1b51-491d-8195-5cea9e91df46",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "- The notebook is running in the Kaggle environment. It is not necessary to download any files.  \n",
    "You can download the files if you want to create a notebook which is running locally on your PC.\n",
    "- Hyperparameter Tuning which could be done:\n",
    "  - More epochs\n",
    "  - Different Learning Rates for Generator and/or Discrimninator\n",
    "  - Larger batches\n",
    "  - Other loss functions\n",
    "  - Different number of filters\n",
    "  - Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33204e-f1b0-4942-9781-269efeaed404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Link GitHub Repository\n",
    "https://github.com/Oliver-VG/GAN-Monet-Kaggle-Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09c65a-b0e5-4ecb-815d-ff38f462dd43",
   "metadata": {},
   "source": [
    "## Citation / References\n",
    "- Kaggle competition: https://www.kaggle.com/competitions/gan-getting-started/overview\n",
    "- https://arxiv.org/abs/1406.2661"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb3893-8a88-4a71-af82-f72933045960",
   "metadata": {},
   "source": [
    "## AI Acknowledgement\n",
    "ChatGPT-5.1 (OpenAI, 2025) was used to assist in proofreading and improving the grammatical accuracy of the Markdown content. No substantive changes to the original ideas or analysis were made by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graph)",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
